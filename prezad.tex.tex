% Created 2019-11-27 Wed 04:00
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.3)}, 
 pdflang={English}}
\begin{document}

\tableofcontents

\documentclass[smaller]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usetheme{Madrid}
\date{\today}
\title{Efficient differentiable programming}
\hypersetup\{
 pdfauthor=\{\},
 pdftitle=\{Efficient differentiable programming\},
 pdfkeywords=\{\},
 pdfsubject=\{\},
 pdfcreator=\{Emacs 26.3 (Org mode 9.3)\},
 pdflang=\{English\}\}

\begin{document}

\maketitle
\begin{frame}{Outline}
\tableofcontents
\end{frame}

\begin{frame}{intro}
\begin{block}{abstract}
\begin{itemize}
\item an AD system for higher order functional array-processing language.
\item supports both source-to-source forward mode AD and global optimizations
\item gradient computation with forward AD can be as efficient as reverse mode
\item Jacobian matrices needed for practical numerical schemes can be efficiently computed
\end{itemize}
The key contribution is in making traditional forward mode AD efficient
through a number of compile time optimizing transformations, e.g. fusion
\end{block}
\end{frame}

\begin{frame}{AD in general}
Two main mode for AD:
\begin{itemize}
\item forward mode computes the derivative of the original computation while making a forward pass (across the computational graph)
\item Reverse mode does a forward pass and then a backwards one to compute the adjoint derivatives.
\end{itemize}
For reverse mode  we associate to every variable $$ v_i $$ its adjoint $$ \frac{\partial y}{\partial v_i} $$
\end{frame}

\begin{frame}[label={sec:org075497c}]{AD in ML}
We want a constant time computation of differentiation as an important
assumption in algorithms is often that the gradient of a function has the
same asymptotic computational cost as the original function.

Most algorithms in optimization have their computational complexity defined in
the number of calls to an \emph{oracle}. In \alert{first order methods} the oracle
is usually either gradient, a subgradient, or maybe some funky proximal
operator.
\end{frame}

\begin{frame}[label={sec:orgeba2a44}]{ML}
\begin{block}{supervised learning setup}
AI is pretty simple nowadays, it's all machine learning, which means
given a dataset of N samples of the form
$$ \{(x_1,y_1), \ldots , (x_{N}, y_{N})\} $$

where the x's are the \emph{feature vectors}  and the y's the result of the prediction,
we wish to find a good predictor, i.e. a function
$$ g : X \rightarrow Y $$.

To define the goodness of a predictor we define a \emph{loss function}
$$ f: Y \times Y \rightarrow \mathbb R $$

Define the empirical risk minimization has the average of all the losses over
the sample. Then the best predictor is the one that minimizes the empirical
risk.
\end{block}
\end{frame}

\begin{frame}[label={sec:orgcbdeb75}]{jacobian}
\begin{itemize}
\item In general if $$ f : \mathbb R^n \rightarrow \mathbb R^m $$ we define the Jacobian the following way:
\end{itemize}


\begin{equation}
J_f = \frac{\partial f}{\partial x} = \begin{bmatrix}
\frac{\partial f_{1}}{\partial x_{1}} & \cdots &
\frac{\partial f_{1}}{\partial x_{n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_{m}}{\partial  x_{1} } & \cdots &
\frac{\partial f_{m}}{\partial x_{n}}
\end{bmatrix}
\end{equation}

Forward mode computes the Jacobian through columns whereas reverse mode computes rows.

Forward mode is efficient when there are many more rows than columns and vice
versa for reverse mode. Reverse mode is thus good for very large dimensions
in the input. Choosing the optimal mix between the two  is NP complete.
\end{frame}


\begin{frame}[label={sec:orgf85f0f0}]{remarks on the jacobian}
\begin{itemize}
\item When computing the matrix there are many loop invariant computations, this
calls for \emph{loop-invariant code motion}
\item Even if the asymptotic computational complexity of a gradient call
is the same as one for the original function the hidden constant
can be quite big in practice. Many intermediate vectors can be
removed through \alert{deforestation} aka \alert{loop fusion}.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org1116535}]{overview}
\begin{center}
\includegraphics[width=.9\line width]{pipeline.png}
\end{center}
\end{frame}
\end{document}
\end{document}
